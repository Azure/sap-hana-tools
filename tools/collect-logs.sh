#!/bin/bash

# This script will capture log files from a specified job when a specific
# event occurs. The basic flow is:
#
#   1. Inspect log files to get count of specific number of events
#   2. Run function start_jobs for specific period of time
#   3. Stop jobs
#   4. Inspect log files to get count of specific number of events.
#      If different from #1, then save all log files generated by start_jobs
#   5. Delete log files from last run in current directory
#   6. Dump to step #1 to collect new set of logs
#
# In this way, if a specific event occurs, we can capture logs to attempt to
# diagnose the issue.
#
# This script creates a lock when running. Thus, it can be started by cron
# or by a system startup script. Multiple jobs will never be running
# simultaneously.
#
# NOTE: If this script is terminated (due to system crash/restart/whatever),
# then the lock file will be left around, causing the program to believe that
# an instance is already running. This lock file should be deleted on startup.
# The best way to handle this is to launch via cron at system boot (using the
# "@reboot" extension), delete the lock file, and then run the script.

# Define variables

DUMP_BASE="/tmp/collect-logs"           # Base directory
DUMP_DIR="${DUMP_BASE}/current"         # Current log directory
LOCK_FILE="${DUMP_BASE}.lock"           # Lock file
TCPDUMP_OUTPUT="${DUMP_DIR}/tcpdump.out" # Output file

SEARCH_FILENAME="/var/log/messages"     # Filename to search for error message

# The search filter is used to search ${SEARCH_FILENAME}. Note that we use
# the `grep` command with -E (allowing regular expressions). We are matching
# a line like:
# 2020-03-25T16:12:14.028479+00:00 hostname kernel: [11785727.694971] nfs: server 10.0.0.10 not responding, still trying
SEARCH_FILTER="nfs: server .* not responding, still trying"

SLEEP_DURATION=300                      # Number of seconds to sleep while start_jobs is running
TCPDUMP_INTERFACE="any"                 # Network interface to capture


# Function definitions

function start_jobs() {
  # Start collecting our data

  /usr/sbin/tcpdump -s 96 -W 10 -C 512 -i "${TCPDUMP_INTERFACE}" -w "${TCPDUMP_OUTPUT}" 2>/dev/null &
}

function stop_jobs() {
  # Delete the collection processes

  killall -w tcpdump
}

function ctrl_c() {
  # Delete the collection processes and exit

  stop_jobs

  rm -f "${LOCK_FILE}"
  exit 0
}

(
  # Get an exclusive lock
  flock -n 9 || exit 1

  # We have an exclusive lock, so get started

  # We want to keep up to one old directory; estimates are that this
  # directory can take up to about 110MB (worst case), which isn't
  # excessive. The concern is that if the system does crash, we may
  # want the data at the time of the crash.

  if [ ! -e "${DUMP_DIR}" ]; then
    mkdir -p "${DUMP_DIR}"
  fi
  rm -rf ${DUMP_DIR:?}/*

  # Start collecting our data

  COUNT_EVENTS=$( grep -cE "${SEARCH_FILTER}" "${SEARCH_FILENAME}" )
  start_jobs

  # Trap for interrupt (control-c)

  trap ctrl_c INT

  # Loop for log checks/log saves

  for (( ; ; )); do
    # Sleep for a while to give time for event to occur
    sleep ${SLEEP_DURATION}

    # Check to see if an event has occurred
    # it is safest to stop collection, check, and start collection again

    stop_jobs > /dev/null 2>&1

    NEW_COUNT=$( grep -cE "${SEARCH_FILTER}" "${SEARCH_FILENAME}" )

    if [ "${COUNT_EVENTS}" -lt "${NEW_COUNT}" ]; then
      # Create a dated directory with our date, like "2020-03-25T19:44:51+00:00"
      SAVE_DIR="${DUMP_BASE}/$( date --iso-8601=seconds )"
      mkdir -p "${SAVE_DIR}"

      cp ${DUMP_DIR}/* "${SAVE_DIR}/"
    fi

    rm ${DUMP_DIR}/*

    # Start the jobs again
    COUNT_EVENTS=${NEW_COUNT}
    start_jobs
  done

) 9>"${LOCK_FILE}"

exit 0
